{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7517413,"sourceType":"datasetVersion","datasetId":4378788},{"sourceId":7518092,"sourceType":"datasetVersion","datasetId":4379268}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dm_control\n!pip install pink-noise-rl\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gym import core, spaces\nfrom dm_control import suite\nfrom dm_env import specs\nimport numpy as np\n\n\ndef _spec_to_box(spec, dtype):\n    def extract_min_max(s):\n        assert s.dtype == np.float64 or s.dtype == np.float32\n        dim = int(np.prod(s.shape))\n        if type(s) == specs.Array:\n            bound = np.inf * np.ones(dim, dtype=np.float32)\n            return -bound, bound\n        elif type(s) == specs.BoundedArray:\n            zeros = np.zeros(dim, dtype=np.float32)\n            return s.minimum + zeros, s.maximum + zeros\n\n    mins, maxs = [], []\n    for s in spec:\n        mn, mx = extract_min_max(s)\n        mins.append(mn)\n        maxs.append(mx)\n    low = np.concatenate(mins, axis=0).astype(dtype)\n    high = np.concatenate(maxs, axis=0).astype(dtype)\n    assert low.shape == high.shape\n    return spaces.Box(low, high, dtype=dtype)\n\n\ndef _flatten_obs(obs):\n    obs_pieces = []\n    for v in obs.values():\n        flat = np.array([v]) if np.isscalar(v) else v.ravel()\n        obs_pieces.append(flat)\n    return np.concatenate(obs_pieces, axis=0)\n\n\nclass DMCWrapper(core.Env):\n    def __init__(\n        self,\n        domain_name,\n        task_name,\n        task_kwargs=None,\n        visualize_reward=False,\n        from_pixels=False,\n        height=84,\n        width=84,\n        camera_id=0,\n        frame_skip=1,\n        environment_kwargs=None,\n        channels_first=True\n    ):\n#         assert 'random' in task_kwargs, 'please specify a seed, for deterministic behaviour'\n        self._from_pixels = from_pixels\n        self._height = height\n        self._width = width\n        self._camera_id = camera_id\n        self._frame_skip = frame_skip\n        self._channels_first = channels_first\n\n        # create task\n        self._env = suite.load(\n            domain_name=domain_name,\n            task_name=task_name,\n            task_kwargs=task_kwargs,\n            visualize_reward=visualize_reward,\n            environment_kwargs=environment_kwargs\n        )\n\n        # true and normalized action spaces\n        self._true_action_space = _spec_to_box([self._env.action_spec()], np.float32)\n        self._norm_action_space = spaces.Box(\n            low=-1.0,\n            high=1.0,\n            shape=self._true_action_space.shape,\n            dtype=np.float32\n        )\n\n        # create observation space\n        if from_pixels:\n            shape = [3, height, width] if channels_first else [height, width, 3]\n            self._observation_space = spaces.Box(\n                low=0, high=255, shape=shape, dtype=np.uint8\n            )\n        else:\n            self._observation_space = _spec_to_box(\n                self._env.observation_spec().values(),\n                np.float64\n            )\n            \n        self._state_space = _spec_to_box(\n            self._env.observation_spec().values(),\n            np.float64\n        )\n        \n        self.current_state = None\n\n        # set seed\n        self.seed(seed=996)\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    def _get_obs(self, time_step):\n        if self._from_pixels:\n            obs = self.render(\n                height=self._height,\n                width=self._width,\n                camera_id=self._camera_id\n            )\n            if self._channels_first:\n                obs = obs.transpose(2, 0, 1).copy()\n        else:\n            obs = _flatten_obs(time_step.observation)\n        return obs\n\n    def _convert_action(self, action):\n        action = action.astype(np.float64)\n        true_delta = self._true_action_space.high - self._true_action_space.low\n        norm_delta = self._norm_action_space.high - self._norm_action_space.low\n        action = (action - self._norm_action_space.low) / norm_delta\n        action = action * true_delta + self._true_action_space.low\n        action = action.astype(np.float32)\n        return action\n\n    @property\n    def observation_space(self):\n        return self._observation_space\n\n    @property\n    def state_space(self):\n        return self._state_space\n\n    @property\n    def action_space(self):\n        return self._norm_action_space\n\n    @property\n    def reward_range(self):\n        return 0, self._frame_skip\n\n    def seed(self, seed):\n        self._true_action_space.seed(seed)\n        self._norm_action_space.seed(seed)\n        self._observation_space.seed(seed)\n\n    def step(self, action):\n        assert self._norm_action_space.contains(action)\n        action = self._convert_action(action)\n        assert self._true_action_space.contains(action)\n        reward = 0\n        extra = {'internal_state': self._env.physics.get_state().copy()}\n\n        for _ in range(self._frame_skip):\n            time_step = self._env.step(action)\n            reward += time_step.reward or 0\n            done = time_step.last()\n            if done:\n                break\n        obs = self._get_obs(time_step)\n        self.current_state = _flatten_obs(time_step.observation)\n        extra['discount'] = time_step.discount\n        return obs, reward, done, extra\n\n    def reset(self):\n        time_step = self._env.reset()\n        self.current_state = _flatten_obs(time_step.observation)\n        obs = self._get_obs(time_step)\n        return obs\n\n    def render(self, mode='rgb_array', height=None, width=None, camera_id=0):\n        assert mode == 'rgb_array', 'only support rgb_array mode, given %s' % mode\n        height = height or self._height\n        width = width or self._width\n        camera_id = camera_id or self._camera_id\n        return self._env.physics.render(\n            height=height, width=width, camera_id=camera_id\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = DMCWrapper(\"walker\",\"run\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport torch\nfrom pink import PinkNoiseDist\nfrom pink import ColoredNoiseDist\nfrom stable_baselines3 import SAC\nimport time\nfrom tqdm import tqdm\n\n# Define a function to evaluate an episode\ndef evaluate_episode(model, env):\n    obs = env.reset()\n    done = False\n    total_reward = 0.0\n    steps=0\n    while steps<1000 and not done:\n        action, _ = model.predict(obs, deterministic=True)\n        obs, reward, done, _ = env.step(action)\n        total_reward += reward\n        steps+=1\n    return total_reward\n\n# Reproducibility\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrng = np.random.default_rng(seed)\n\n# Initialize environment\n\naction_dim = env.action_space.shape[-1]\nseq_len = 1000\nrng = np.random.default_rng(0)\n\n# Initialize agents\nmodel_default = SAC(\"MlpPolicy\", env, seed=seed)\nmodel_pink = SAC(\"MlpPolicy\", env, seed=seed)\nmodel_OU = SAC(\"MlpPolicy\", env, seed=seed)\n\n# Set action noise\nmodel_pink.actor.action_dist = PinkNoiseDist(seq_len, action_dim, rng=rng)\nmodel_OU.actor.action_dist = ColoredNoiseDist(beta=2, seq_len=seq_len, action_dim=action_dim, rng=rng)\n\n# Training parameters\ntotal_timesteps = 1000000\neval_frequency = 10000 # Evaluate every 104 interactions\neval_rollouts = 5\n\nwandb.init(\n    project=\"Pinkie\",\n    config = {\n    \"Total_timesteps\": total_timesteps,\n    \"Eval_frequency\": eval_frequency,\n    \"Eval_rollouts\": eval_rollouts\n    }\n)\n\n#Final average performances\navg_default=0.0\navg_pink=0.0\navg_OU=0.0\nfinal_default=0.0\nfinal_pink=0.0\nfinal_OU=0.0\n\n# Train agents with evaluation\ntimesteps_so_far = 0\n# for timesteps_so_far in tqdm(range(0, total_timesteps, eval_frequency)):\nwhile timesteps_so_far < total_timesteps:\n    t1 = time.time()\n    # Train the default noise model\n    model_default.learn(total_timesteps=eval_frequency)\n    t2 = time.time()\n\n    # Evaluate the default noise model\n    mean_return_default = 0.0\n    for _ in range(eval_rollouts):\n        mean_return_default += evaluate_episode(model_default, env)\n    mean_return_default /= eval_rollouts\n    avg_default+=mean_return_default\n    if(timesteps_so_far>=0.95*total_timesteps):\n        final_default+=mean_return_default\n\n    print(f\"Return (Default): {mean_return_default}\")\n    print(f\"Time taken (Default Model): {t2 - t1:.2f} seconds\")\n    print(f\"Timesteps: {timesteps_so_far}, Mean Return: {mean_return_default}\")\n\n    t1=time.time()\n    # Train the pink noise model\n    model_pink.learn(total_timesteps=eval_frequency)\n    # timesteps_so_far += eval_frequency\n    t2 = time.time()\n\n    # Evaluate the pink noise model\n    mean_return_pink = 0.0\n    for _ in range(eval_rollouts):\n        mean_return_pink += evaluate_episode(model_pink, env)\n    mean_return_pink /= eval_rollouts\n    avg_pink+=mean_return_pink\n    if(timesteps_so_far>=0.95*total_timesteps):\n        final_pink+=mean_return_pink\n\n    print(f\"Return (Pink): {mean_return_pink}\")\n    print(f\"Time taken (Pink Noise Model): {t2 - t1:.2f} seconds\")\n    print(f\"Timesteps: {timesteps_so_far}, Mean Return: {mean_return_pink}\")\n\n    t1=time.time()\n    # Train the pink noise model\n    model_OU.learn(total_timesteps=eval_frequency)\n    # timesteps_so_far += eval_frequency\n    t2 = time.time()\n    \n    # Evaluate the pink noise model\n    mean_return_OU = 0.0\n    for _ in range(eval_rollouts):\n        mean_return_OU += evaluate_episode(model_OU, env)\n    mean_return_OU/= eval_rollouts\n    avg_OU+=mean_return_OU\n    if(timesteps_so_far>=0.95*total_timesteps):\n        final_OU+=mean_return_OU\n\n    print(f\"Return (OU): {mean_return_OU}\")\n    print(f\"Time taken (OU Noise Model): {t2 - t1:.2f} seconds\")\n    print(f\"Timesteps: {timesteps_so_far}, Mean Return: {mean_return_OU}\")\n\n    timesteps_so_far += eval_frequency\n    \n    wandb.log({\n        \"mean_return_OU\": mean_return_OU,\n        \"mean_return_pink\": mean_return_pink,\n        \"mean_return_default\": mean_return_default,\n        \"timesteps_so_far\": timesteps_so_far\n    })\n\navg_default/=(total_timesteps/eval_frequency)\navg_pink/=(total_timesteps/eval_frequency)\navg_OU/=(total_timesteps/eval_frequency)\n\nfinal_default/=(0.05*total_timesteps/eval_frequency)\nfinal_pink/=(0.05*total_timesteps/eval_frequency)\nfinal_OU/=(0.05*total_timesteps/eval_frequency)\n\nwandb.log({\n    \"final_default\": final_default,\n    \"final_pink\": final_pink,\n    \"final_OU\": final_OU,\n    \"avg_default\": avg_default,\n    \"avg_pink\": avg_pink,\n    \"avg_OU\": avg_OU\n})\n\nprint(\"Mean:\")\nprint(f\"White:{avg_default}           Pink:{avg_pink}             OU:{avg_OU}\")\nprint(\"Final:\")\nprint(f\"White:{final_default}           Pink:{final_pink}             OU:{final_OU}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}