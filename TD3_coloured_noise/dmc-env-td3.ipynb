{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7515662,"sourceType":"datasetVersion","datasetId":4377732}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dm_control\n!pip install pink-noise-rl\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-31T13:12:43.037221Z","iopub.execute_input":"2024-01-31T13:12:43.037494Z","iopub.status.idle":"2024-01-31T13:13:25.841367Z","shell.execute_reply.started":"2024-01-31T13:12:43.037469Z","shell.execute_reply":"2024-01-31T13:13:25.840414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gym\nfrom gym import spaces\n\nfrom dm_control import suite\nfrom dm_env import specs\n\n\ndef convert_dm_control_to_gym_space(dm_control_space):\n    r\"\"\"Convert dm_control space to gym space. \"\"\"\n    if isinstance(dm_control_space, specs.BoundedArray):\n        space = spaces.Box(low=dm_control_space.minimum, \n                           high=dm_control_space.maximum, \n                           dtype=dm_control_space.dtype)\n        assert space.shape == dm_control_space.shape\n        return space\n    elif isinstance(dm_control_space, specs.Array) and not isinstance(dm_control_space, specs.BoundedArray):\n        space = spaces.Box(low=-float('inf'), \n                           high=float('inf'), \n                           shape=dm_control_space.shape, \n                           dtype=dm_control_space.dtype)\n        return space\n    elif isinstance(dm_control_space, dict):\n        space = spaces.Dict({key: convert_dm_control_to_gym_space(value)\n                             for key, value in dm_control_space.items()})\n        return space\n\n\nclass DMSuiteEnv(gym.Env):\n    def __init__(self, domain_name, task_name, task_kwargs=None, environment_kwargs=None, visualize_reward=False):\n        self.env = suite.load(domain_name, \n                              task_name, \n                              task_kwargs=task_kwargs, \n                              environment_kwargs=environment_kwargs, \n                              visualize_reward=visualize_reward)\n        self.metadata = {'render.modes': ['human', 'rgb_array'],\n                         'video.frames_per_second': round(1.0/self.env.control_timestep())}\n        print(self.env.observation_spec())\n        self.observation_space = convert_dm_control_to_gym_space(self.env.observation_spec())\n        print(self.observation_space)\n        print(\"________________________\")\n        print(self.env.action_spec())\n        self.action_space = convert_dm_control_to_gym_space(self.env.action_spec())\n        print(self.action_space)\n        self.viewer = None\n    \n    def seed(self, seed):\n        return self.env.task.random.seed(seed)\n    \n    def step(self, action):\n        timestep = self.env.step(action)\n        observation = timestep.observation\n        reward = timestep.reward\n        done = timestep.last()\n        info = {}\n        truncated = False\n        return observation, reward, done, info\n    \n    def reset(self):\n        timestep = self.env.reset()\n        return timestep.observation\n    \n    def render(self, mode='human', **kwargs):\n        if 'camera_id' not in kwargs:\n            kwargs['camera_id'] = 0  # Tracking camera\n        use_opencv_renderer = kwargs.pop('use_opencv_renderer', False)\n        \n        img = self.env.physics.render(**kwargs)\n        if mode == 'rgb_array':\n            return img\n        elif mode == 'human':\n            if self.viewer is None:\n                if not use_opencv_renderer:\n                    from gym.envs.classic_control import rendering\n                    self.viewer = rendering.SimpleImageViewer(maxwidth=1024)\n                else:\n                    from . import OpenCVImageViewer\n                    self.viewer = OpenCVImageViewer()\n            self.viewer.imshow(img)\n            return self.viewer.isopen\n        else:\n            raise NotImplementedError\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n        return self.env.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env1 = [\"cartpole\", \"cartpole\", \"ball_in_cup\", \"hopper\", \"cheetah\", \"reacher\", \"pendulum\"]\nenv2 = [\"balance_sparse\",\"swingup_sparse\",\"catch\",\"hop\",\"run\",\"hard\",\"swingup\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport torch\nfrom pink import PinkActionNoise\nfrom pink import ColoredActionNoise\nfrom stable_baselines3 import TD3\nimport time\nfrom tqdm import tqdm\n\n# Define a function to evaluate an episode\ndef evaluate_episode(model, env):\n    obs = env.reset()\n    done = False\n    total_reward = 0.0\n    steps=0\n    while steps<1000 and not done:\n        action, _ = model.predict(obs, deterministic=True)\n        obs, reward, done, _ = env.step(action)\n        total_reward += reward\n        steps+=1\n    return total_reward\n\n# Reproducibility\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrng = np.random.default_rng(seed)\n\nfor i in range(7):\n    # Initialize environment\n    env = DMSuiteEnv(env1[i],env2[i])\n    action_dim = env.action_space.shape[-1]\n    seq_len = 1000\n    rng = np.random.default_rng(0)\n\n    # Initialize agents\n    model_default = TD3(\"MultiInputPolicy\", env)\n    model_pink = TD3(\"MultiInputPolicy\", env)\n    model_OU = TD3(\"MultiInputPolicy\", env)\n\n    # Set action noise\n    noise_scale = 0.3\n    model_pink.action_noise = PinkActionNoise(noise_scale, seq_len, action_dim)\n    model_OU.action_noise = ColoredActionNoise(2, noise_scale, seq_len, action_dim)\n\n    # Training parameters\n    total_timesteps = 1000000\n    eval_frequency = 10000 # Evaluate every 104 interactions\n    eval_rollouts = 5\n\n    wandb.init(\n        project=\"Pinkie\",\n        config = {\n        \"Total_timesteps\": total_timesteps,\n        \"Eval_frequency\": eval_frequency,\n        \"Eval_rollouts\": eval_rollouts,\n        \"Environment\": env1[i] + \" \" + env2[i]\n        }\n    )\n\n    #Final average performances\n    avg_default=0.0\n    avg_pink=0.0\n    avg_OU=0.0\n    final_default=0.0\n    final_pink=0.0\n    final_OU=0.0\n\n    # Train agents with evaluation\n    # timesteps_so_far = 0\n    for timesteps_so_far in tqdm(range(total_timesteps)):\n        t1 = time.time()\n        # Train the default noise model\n        model_default.learn(total_timesteps=eval_frequency)\n        t2 = time.time()\n\n        # Evaluate the default noise model\n        mean_return_default = 0.0\n        for _ in range(eval_rollouts):\n            mean_return_default += evaluate_episode(model_default, env)\n        mean_return_default /= eval_rollouts\n        avg_default+=mean_return_default\n        if(timesteps_so_far>=0.95*total_timesteps):\n            final_default+=mean_return_default\n\n        print(f\"Return (Default): {mean_return_default}\")\n        print(f\"Time taken (Default Model): {t2 - t1:.2f} seconds\")\n        print(f\"Timesteps: {timesteps_so_far}, Mean Return: {mean_return_default}\")\n\n        t1=time.time()\n        # Train the pink noise model\n        model_pink.learn(total_timesteps=eval_frequency)\n        # timesteps_so_far += eval_frequency\n        t2 = time.time()\n\n        # Evaluate the pink noise model\n        mean_return_pink = 0.0\n        for _ in range(eval_rollouts):\n            mean_return_pink += evaluate_episode(model_pink, env)\n        mean_return_pink /= eval_rollouts\n        avg_pink+=mean_return_pink\n        if(timesteps_so_far>=0.95*total_timesteps):\n            final_pink+=mean_return_pink\n\n        print(f\"Return (Pink): {mean_return_pink}\")\n        print(f\"Time taken (Pink Noise Model): {t2 - t1:.2f} seconds\")\n        print(f\"Timesteps: {timesteps_so_far}, Mean Return: {mean_return_pink}\")\n\n        t1=time.time()\n        # Train the pink noise model\n        model_OU.learn(total_timesteps=eval_frequency)\n        # timesteps_so_far += eval_frequency\n        t2 = time.time()\n\n        # Evaluate the pink noise model\n        mean_return_OU = 0.0\n        for _ in range(eval_rollouts):\n            mean_return_OU += evaluate_episode(model_OU, env)\n        mean_return_OU/= eval_rollouts\n        avg_OU+=mean_return_OU\n        if(timesteps_so_far>=0.95*total_timesteps):\n            final_OU+=mean_return_OU\n\n        print(f\"Return (OU): {mean_return_OU}\")\n        print(f\"Time taken (OU Noise Model): {t2 - t1:.2f} seconds\")\n        print(f\"Timesteps: {timesteps_so_far}, Mean Return: {mean_return_OU}\")\n\n        timesteps_so_far += eval_frequency\n\n        wandb.log({\n            \"mean_return_OU\": mean_return_OU,\n            \"mean_return_pink\": mean_return_pink,\n            \"mean_return_default\": mean_return_default,\n            \"timesteps_so_far\": timesteps_so_far\n        })\n\n    avg_default/=(total_timesteps/eval_frequency)\n    avg_pink/=(total_timesteps/eval_frequency)\n    avg_OU/=(total_timesteps/eval_frequency)\n\nfinal_default/=(0.05*total_timesteps/eval_frequency)\nfinal_pink/=(0.05*total_timesteps/eval_frequency)\nfinal_OU/=(0.05*total_timesteps/eval_frequency)\n\nwandb.log({\n    \"final_default\": final_default,\n    \"final_pink\": final_pink,\n    \"final_OU\": final_OU,\n    \"avg_default\": avg_default,\n    \"avg_pink\": avg_pink,\n    \"avg_OU\": avg_OU\n})\n\nprint(\"Mean:\")\nprint(f\"White:{avg_default}           Pink:{avg_pink}             OU:{avg_OU}\")\nprint(\"Final:\")\nprint(f\"White:{final_default}           Pink:{final_pink}             OU:{final_OU}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# env = DMSuiteEnv(\"cartpole\",\"balance_sparse\")\n# env = DMSuiteEnv(\"cartpole\",\"swingup_sparse\")\n# env = DMSuiteEnv(\"ball_in_cup\",\"catch\")\n# env = DMSuiteEnv(\"hopper\",\"hop\")\n# env = DMSuiteEnv(\"cheetah\",\"run\")\n# env = DMSuiteEnv(\"cheetah\",\"run\")\n# env = DMSuiteEnv(\"pendulum\",\"swingup\")\n# env = DMSuiteEnv(\"reacher\",\"hard\")","metadata":{},"execution_count":null,"outputs":[]}]}
